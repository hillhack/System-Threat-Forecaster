{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":90791,"databundleVersionId":10592855,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:08:17.069329Z","iopub.execute_input":"2025-02-23T03:08:17.069712Z","iopub.status.idle":"2025-02-23T03:08:17.074699Z","shell.execute_reply.started":"2025-02-23T03:08:17.069680Z","shell.execute_reply":"2025-02-23T03:08:17.073788Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading and Storing the Dataset","metadata":{}},{"cell_type":"code","source":"def load_data():\n    \"\"\"\n    Load the training, test, and sample submission datasets from CSV files.\n    Returns three dataframes: train, test.\n    \"\"\"\n    df = pd.read_csv(\"/kaggle/input/System-Threat-Forecaster/train.csv\")\n    test_data = pd.read_csv(\"/kaggle/input/System-Threat-Forecaster/test.csv\")\n    df = df.reindex(sorted(df.columns), axis=1)\n    test_data = test_data.reindex(sorted(test_data.columns), axis=1)\n\n    return df, test_data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:08:17.076127Z","iopub.execute_input":"2025-02-23T03:08:17.076420Z","iopub.status.idle":"2025-02-23T03:08:17.093352Z","shell.execute_reply.started":"2025-02-23T03:08:17.076391Z","shell.execute_reply":"2025-02-23T03:08:17.092612Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"\ndef exploratory_data_analysis(df):\n    \"\"\"\n    Perform Exploratory Data Analysis (EDA) on the given DataFrame.\n    Includes null value analysis, data type distribution, statistical summary, and more.\n    \"\"\"\n    print(\"Exploratory Data Analysis (EDA)\")\n\n    # Finding columns with null values\n    null_counts = df.isnull().sum()[df.isnull().sum() > 0].sort_values()\n    print(\"Columns with Missing Values:\\n\", null_counts)\n\n    print(\"Data Type Distribution:\")\n    print(df.dtypes.value_counts())\n\n    print(\"asic Statistical Summary (Numerical Data):\")\n    print(df.describe())\n\n    print(\"Duplicate Rows Count:\", df.duplicated().sum())\n    print(\"EDA Completed!\")\ndf, test_data = load_data()\nexploratory_data_analysis(df)   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:08:17.094550Z","iopub.execute_input":"2025-02-23T03:08:17.094800Z","iopub.status.idle":"2025-02-23T03:08:19.014959Z","shell.execute_reply.started":"2025-02-23T03:08:17.094781Z","shell.execute_reply":"2025-02-23T03:08:19.013942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(3, 2))\ntarget_dis = df['target'].value_counts()\n\nplt.pie(target_dis, labels=target_dis.index, autopct='%1.1f%%', startangle=90, \n        wedgeprops={'edgecolor': 'black'})\n\nplt.title(\"Target Variable Distribution\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:08:19.016399Z","iopub.execute_input":"2025-02-23T03:08:19.016738Z","iopub.status.idle":"2025-02-23T03:08:19.098339Z","shell.execute_reply.started":"2025-02-23T03:08:19.016706Z","shell.execute_reply":"2025-02-23T03:08:19.097391Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Handling Missing Data","metadata":{}},{"cell_type":"code","source":"def handle_missing_values(df, test_data):\n    \"\"\"\n    Handles missing values:\n    - Replaces numerical NaNs with column mean.\n    - Replaces categorical NaNs with the most frequent value.\n    \"\"\"\n    num_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.drop(\"target\", errors=\"ignore\")\n    cat_cols = df.select_dtypes(include=[\"object\"]).columns\n\n    num_imputer = SimpleImputer(strategy=\"mean\")\n    cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n\n    df[num_cols] = num_imputer.fit_transform(df[num_cols])\n    test_data[num_cols] = num_imputer.transform(test_data[num_cols])\n\n    df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n    test_data[cat_cols] = cat_imputer.transform(test_data[cat_cols])\n\n    print(\"Missing values handled successfully.\")\n    return df, test_data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:08:19.099327Z","iopub.execute_input":"2025-02-23T03:08:19.099650Z","iopub.status.idle":"2025-02-23T03:08:19.105333Z","shell.execute_reply.started":"2025-02-23T03:08:19.099617Z","shell.execute_reply":"2025-02-23T03:08:19.104411Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def extract_time_features(df, test_data):\n    \"\"\"\n    Extracts the month from the 'DateAS' column.\n    \"\"\"\n    df[\"DateAS\"] = pd.to_datetime(df[\"DateAS\"])\n    test_data[\"DateAS\"] = pd.to_datetime(test_data[\"DateAS\"])\n    df[\"Month\"] = df[\"DateAS\"].dt.month\n    test_data[\"Month\"] = test_data[\"DateAS\"].dt.month\n    df.drop(columns=[\"DateAS\"], inplace=True)\n    test_data.drop(columns=[\"DateAS\"], inplace=True)\n    print(\"Time features extracted successfully.\")\n\n    return df, test_data\n\ndef encode_categorical_features(df, test_data):\n    \"\"\"Encodes categorical features: One-hot encoding for low cardinality, Frequency encoding for high cardinality.\"\"\"\n    categorical_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n    low_cardinality_features = [col for col in categorical_cols if df[col].nunique() <= 20]\n    high_cardinality_features = [col for col in categorical_cols if df[col].nunique() > 20]\n\n    # One-Hot Encoding for low cardinality\n    df = pd.get_dummies(df, columns=low_cardinality_features, drop_first=True)\n    test_data = pd.get_dummies(test_data, columns=low_cardinality_features, drop_first=True)\n\n    df= df.drop(columns=list(set(df.columns)-(set(test_data.columns)|{'target'})), axis=1)\n\n    # Frequency Encoding for high cardinality\n    for col in high_cardinality_features:\n        freq = df[col].value_counts()\n        df[col + 'freq'] = df[col].map(freq)\n        freq_test = test_data[col].value_counts()\n        test_data[col + 'freq'] = test_data[col].map(freq_test)\n\n    # Drop original high-cardinality categorical features\n    df.drop(columns=high_cardinality_features, inplace=True, errors=\"ignore\")\n    test_data.drop(columns=high_cardinality_features, inplace=True, errors=\"ignore\")\n    print(\"categorical features encoded successfully.\")\n\n    return df, test_data\n\ndef scale_features(df, test_data):\n    \"\"\"\n    Scales numerical features using RobustScaler.\n    \"\"\"\n    num_cols = df.select_dtypes(include=[\"int64\",'int32', \"float64\"]).columns.drop(\"target\", errors=\"ignore\")\n    scaler = RobustScaler()\n\n    df[num_cols] = scaler.fit_transform(df[num_cols])\n    test_data[num_cols] = scaler.fit_transform(test_data[num_cols])\n    print(\"Features scaled successfully.\")\n    return df, test_data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:08:19.106402Z","iopub.execute_input":"2025-02-23T03:08:19.106610Z","iopub.status.idle":"2025-02-23T03:08:19.129335Z","shell.execute_reply.started":"2025-02-23T03:08:19.106592Z","shell.execute_reply":"2025-02-23T03:08:19.128508Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Engineering and Extraction","metadata":{}},{"cell_type":"code","source":"\ndef remove_low_cardinality_features(df, test_df=None):\n    \"\"\"\n    Removes features with only a single unique value (cardinality == 1).\n    pd.DataFrame, pd.DataFrame: Processed train and test dataframes\n    \"\"\"\n    cardinality = df.nunique()\n    low_cardinality = cardinality[cardinality == 1].index.tolist()\n\n    df = df.drop(columns=low_cardinality)\n    test_df = test_df.drop(columns=low_cardinality)\n\n    return df, test_df\n\n\ndef remove_high_correlation_features(df):\n    \"\"\"\n    Removes features that are highly correlated (correlation > threshold).\n    \"\"\"\n    corr_matrix = df.corr().abs()\n    corr_pairs = corr_matrix.unstack().sort_values(ascending=False)\n    high_corr_pairs = corr_pairs[(corr_pairs < 1) & (corr_pairs > 0.993)]\n    features_to_drop = set()\n    for feat1, feat2 in high_corr_pairs.index:\n        if feat1 not in features_to_drop and feat2 not in features_to_drop:\n            features_to_drop.add(feat2)  # Keep feat1, drop feat2\n\n    df = df.drop(columns=features_to_drop)\n    print(f\"Removed highly correlated pair features.\")\n    return df\n\ndef filter_low_correlation_features(df, target=\"target\", threshold=0.005):\n    \"\"\"\n    Retains only features with absolute correlation >= threshold with the target variable.\n    \"\"\"\n    print(f\"Removed low correlated features.\")\n    filtered_corr = df.corr()[target].loc[lambda x: abs(x) >= threshold]\n    df = df[filtered_corr.index]\n    \n    return df\n\n\ndef clean_column_names(df, test_data):\n    \"\"\"\n    Cleans column names by replacing special characters with underscores.\n    \"\"\"\n    df.columns = df.columns.str.replace(\"[^a-zA-Z0-9]\", \"_\", regex=True)\n    test_data.columns = test_data.columns.str.replace(\"[^a-zA-Z0-9]\", \"_\", regex=True)\n    \n    print(\"Column names cleaned successfully.\")\n    return df, test_data\n\ndef feature_selection_variance(X, threshold=0.0005):\n    \"\"\"\n    Removes low-variance features.\n    \"\"\"\n    selector = VarianceThreshold(threshold=threshold)\n    X_selected = selector.fit_transform(X)\n    selected_columns = X.columns[selector.get_support()]\n    \n    print(\"Low-variance features removed successfully.\")\n    return pd.DataFrame(X_selected, columns=selected_columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:08:19.130204Z","iopub.execute_input":"2025-02-23T03:08:19.130482Z","iopub.status.idle":"2025-02-23T03:08:19.153852Z","shell.execute_reply.started":"2025-02-23T03:08:19.130458Z","shell.execute_reply":"2025-02-23T03:08:19.153121Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Splitting Data","metadata":{}},{"cell_type":"code","source":"def prepare_final_datasets(df, test_data, target=\"target\"):\n    \"\"\"feature selection, and splits data.\n    Returns:\n    - X_train, X_test, y_train, y_test: Train-Test split\n    - test_data: Processed test dataset\n    \"\"\"\n    # Keep only common features in both df and test_data\n    common_features = list(set(df.columns) & set(test_data.columns))\n    \n    df = df[common_features + [target]]  # Retain target column in train dataset\n    test_data = test_data[common_features]  # Keep only aligned features in test dataset\n    # Separate features and target variable\n    X = df.drop(columns=[target])\n    y = df[target]\n\n    X = feature_selection_variance(X)\n   \n    # Split into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    return X_train, X_test, y_train, y_test, test_data ,X,y,df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:08:19.156612Z","iopub.execute_input":"2025-02-23T03:08:19.156892Z","iopub.status.idle":"2025-02-23T03:08:19.174645Z","shell.execute_reply.started":"2025-02-23T03:08:19.156867Z","shell.execute_reply":"2025-02-23T03:08:19.173793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_pipeline(df,test_data):\n    \"\"\"\n    Runs the entire preprocessing pipeline in sequence.\n    \"\"\"\n    df, test_data = remove_low_cardinality_features(df, test_data)\n    df, test_data = handle_missing_values(df, test_data)\n    df, test_data = extract_time_features(df, test_data)\n    df, test_data = encode_categorical_features(df, test_data)\n    df, test_data = scale_features(df, test_data)\n    df = remove_high_correlation_features(df)\n    df = filter_low_correlation_features(df, target=\"target\", threshold=0.005)\n    df, test_data = clean_column_names(df, test_data)\n\n    return prepare_final_datasets(df, test_data, target=\"target\")\n# Run the preprocessing pipeline\nX_train, X_test, y_train, y_test, test_data,X,y,df= preprocess_pipeline(df,test_data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:08:19.175669Z","iopub.execute_input":"2025-02-23T03:08:19.175925Z","iopub.status.idle":"2025-02-23T03:08:33.492428Z","shell.execute_reply.started":"2025-02-23T03:08:19.175906Z","shell.execute_reply":"2025-02-23T03:08:33.491498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def feature_selection_lgb(X, y,X_train, y_train):\n    \"\"\"\n    Uses LGBMClassifier to determine feature importance and removes features below a threshold.\n    \"\"\"\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Train LightGBM Model\n    lgbm = LGBMClassifier(n_estimators=300, learning_rate=0.08, verbose=-1, random_state=42)\n    lgbm.fit(X_train, y_train)\n    \n    # Get feature importance\n    importances = lgbm.feature_importances_\n    \n    # Create DataFrame for feature importance\n    feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n    \n    # Sort by importance (descending)\n    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\n    # Display top 20 most important features\n    print(\"Top 20 Most Important Features:\")\n    print(feature_importance_df.head(20))\n\n    # Plot Top 30 Features\n    top_30_features = feature_importance_df.head(30)\n\n    # Ensure correct plot scaling\n    plt.figure(figsize=(12, 8))\n    sns.barplot(x='Importance', y='Feature', data=top_30_features, palette='viridis')\n    plt.xlabel('Feature Importance Score', fontsize=12)\n    plt.ylabel('Features', fontsize=12)\n    plt.title('Top 30 Most Important Features in LightGBM', fontsize=14)\n    plt.grid(axis='x', linestyle='--', alpha=0.7)  # Adds grid lines for better readability\n    plt.show()\n\n    # Select Important Features\n    selected_features = feature_importance_df[feature_importance_df['Importance'] > 0]['Feature']\n    X_selected = X[selected_features]  # Avoid modifying X inplace\n\n    # Split Data Again After Feature Selection\n    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n\n    return X_selected, X_train, X_test, y_train, y_test\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:09:50.473974Z","iopub.execute_input":"2025-02-23T03:09:50.474314Z","iopub.status.idle":"2025-02-23T03:09:50.481198Z","shell.execute_reply.started":"2025-02-23T03:09:50.474290Z","shell.execute_reply":"2025-02-23T03:09:50.480122Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, X_train, X_test, y_train, y_test= feature_selection_lgb(X, y ,X_train, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:09:57.514222Z","iopub.execute_input":"2025-02-23T03:09:57.514513Z","iopub.status.idle":"2025-02-23T03:10:00.180279Z","shell.execute_reply.started":"2025-02-23T03:09:57.514492Z","shell.execute_reply":"2025-02-23T03:10:00.179315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\n\n# Define number of plots per row\nnum_cols = 4  \nnum_rows = math.ceil(len(X.columns) / num_cols)\n\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5 * num_rows))\naxes = axes.flatten()  # Flatten axes for easy indexing\n\nfor i, fe in enumerate(X.columns):\n    sns.kdeplot(df[df['target'] == 0][fe], label=\"Class 0\", fill=True, color=\"blue\", ax=axes[i])\n    sns.kdeplot(df[df['target'] == 1][fe], label=\"Class 1\", fill=True, color=\"red\", ax=axes[i])\n    \n    axes[i].set_title(f\"KDE Plot of {fe}\")\n    axes[i].set_xlabel(fe)\n    axes[i].set_ylabel(\"Density\")\n    axes[i].legend()  # Add legend\n\n# Remove empty subplots if the number of features is not a multiple of num_cols\nfor j in range(i + 1, len(axes)):  \n    fig.delaxes(axes[j])  \n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:08:33.532990Z","iopub.status.idle":"2025-02-23T03:08:33.533268Z","shell.execute_reply":"2025-02-23T03:08:33.533165Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Evaluation and Comparison","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\ndef evaluate_model(name, y_true, y_pred):\n    \"\"\"\n    Evaluates a model's performance using various metrics and prints the results.\n    \"\"\"\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    roc_auc = roc_auc_score(y_true, y_pred)\n\n    print(f\"\\n🔹 {name} Performance:\")\n    print(f\"   Accuracy:  {accuracy:.4f}\")\n    print(f\"   Precision: {precision:.4f}\")\n    print(f\"   Recall:    {recall:.4f}\")\n    print(f\"   F1-score:  {f1:.4f}\")\n    print(f\"   ROC-AUC:   {roc_auc:.4f}\")\n    print(\"-\" * 40)\n\ndef compare_models(X_train, y_train, X_test, y_test):\n    \"\"\"\n    Trains and evaluates LightGBM, CatBoost, and XGBoost models, then prints their performance.\n    \"\"\"\n    models = {\n        \"LightGBM\": LGBMClassifier(n_estimators=500, learning_rate=0.09, colsample_bytree=0.7, random_state=42, verbose=-1, n_jobs=-1),\n        \"CatBoost\": CatBoostClassifier(iterations=500, learning_rate=0.09,verbose=0, random_seed=42, thread_count=-1),\n        \"XGBoost\": XGBClassifier(n_estimators=500, learning_rate=0.09,colsample_bytree=0.7, random_state=42, n_jobs=-1)\n    }\n    \n    for name, model in models.items():\n        model.fit(X_train, y_train)  # Train\n        y_pred = model.predict(X_test)  # Predict\n        evaluate_model(name, y_test, y_pred)  # Evaluate\n\ncompare_models(X_train, y_train, X_test, y_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:08:33.533938Z","iopub.status.idle":"2025-02-23T03:08:33.534237Z","shell.execute_reply":"2025-02-23T03:08:33.534127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom scipy.stats import randint, uniform\n\n\ndef hyperparameter_tuning(X_train, y_train):\n    \"\"\"Optimized RandomizedSearchCV for XGBoost using scipy.stats distributions.\"\"\"\n    \n    param_dist = {\n        'n_estimators': randint(500, 1000),  # More trees for better learning\n        'learning_rate': uniform(0.01, 0.03),  # Wider learning rate range\n        'colsample_bytree':[0.7]\n    }\n\n    xgb = XGBClassifier(random_state=42, verbosity=0, n_jobs=-1)\n\n    random_search = RandomizedSearchCV(\n        xgb, param_dist, n_iter=3, cv=2, scoring='accuracy', n_jobs=-1, verbose=1, random_state=42\n    )\n\n    random_search.fit(X_train, y_train)\n    print(\"Best Parameters:\", random_search.best_params_)\n\n    return XGBClassifier(**random_search.best_params_, random_state=42, verbosity=0)\n\ndef train_and_evaluate(X_train, y_train, X_test, y_test):\n    \"\"\"Trains the best XGBoost model and evaluates it.\"\"\"\n    best_model = hyperparameter_tuning(X_train, y_train)\n    best_model.fit(X_train, y_train)\n    \n    y_pred = best_model.predict(X_test)\n    evaluate_model(\"XGBoost\", y_test, y_pred)\n    return best_model\n\nmodel = train_and_evaluate(X_train, y_train, X_test, y_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:08:33.535024Z","iopub.status.idle":"2025-02-23T03:08:33.535388Z","shell.execute_reply":"2025-02-23T03:08:33.535272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = test_data[X.columns]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:08:33.536000Z","iopub.status.idle":"2025-02-23T03:08:33.536315Z","shell.execute_reply":"2025-02-23T03:08:33.536211Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Submission","metadata":{}},{"cell_type":"code","source":"test_predictions =model.predict(test_data)  # Generate predictions\nsample_submission = pd.read_csv('/kaggle/input/System-Threat-Forecaster/sample_submission.csv')\nsubmission = sample_submission.copy()\nsubmission['target'] = test_predictions  \n\nsubmission['target'] = submission['target'].apply(lambda x: 1 if x > 0.5 else 0)\n\n# Save to CSV\nsubmission.to_csv('submission.csv', index=False)\n\n# Output file ready for submission\nprint(\"Submission file created.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T03:08:33.536881Z","iopub.status.idle":"2025-02-23T03:08:33.537262Z","shell.execute_reply":"2025-02-23T03:08:33.537093Z"}},"outputs":[],"execution_count":null}]}
